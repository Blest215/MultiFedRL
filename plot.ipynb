{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adde201",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "px = 1 / plt.rcParams['figure.dpi']\n",
    "\n",
    "sns.set_theme(\n",
    "    style='darkgrid',\n",
    "    font_scale=2,\n",
    "    rc={\n",
    "        'figure.figsize': (1920 * px, 1080 * px),\n",
    "        'axes.labelsize': 48,\n",
    "        'xtick.labelsize': 36,\n",
    "        'ytick.labelsize': 36,\n",
    "        'legend.fontsize': 36,\n",
    "        'grid.linewidth': 2,\n",
    "    }\n",
    ")\n",
    "\n",
    "datecode = os.path.basename(os.getcwd())\n",
    "print(f\"Experiment code: {datecode}\")\n",
    "\n",
    "configuration = json.load(open(f'configuration_{datecode}.txt'))\n",
    "\n",
    "def get_dataframe():\n",
    "    return pd.read_csv(f'data_{datecode}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a1cab",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e808a8",
   "metadata": {},
   "source": [
    "## Convergence speeds over the training simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe()\n",
    "\n",
    "window_size = 100\n",
    "agents = ['MultiFedRL', 'FlexiFed', 'ClusterFed', 'Independent']\n",
    "\n",
    "df = df[(df['Train'] == True) & (df['Agent'] != 'Random')]\n",
    "\n",
    "df = df.groupby(['Agent', 'Experiment', 'Environment', 'Iteration']).rolling(window=window_size, min_periods=1, on='Step').mean()\n",
    "\n",
    "plot = sns.lineplot(\n",
    "    data=df, x='Step', y='Rewards', hue='Agent', style='Agent', style_order=agents, hue_order=agents,\n",
    "    markers=True, markersize=24, linewidth=3, markevery=100,\n",
    ")\n",
    "\n",
    "plot.legend(facecolor='white', loc='lower right', markerscale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe()\n",
    "\n",
    "window_size = 100\n",
    "threshold = 0.9\n",
    "agents = ['Independent', 'ClusterFed', 'FlexiFed', 'MultiFedRL']\n",
    "\n",
    "num_experiments = df['Experiment'].max() + 1\n",
    "num_environments = configuration['num_environments']\n",
    "\n",
    "# Get the final performance\n",
    "test_df = df[(df['Train'] == False)]\n",
    "test_df = test_df.groupby(['Agent', 'Experiment','Iteration']).mean()\n",
    "test_df = test_df.groupby(['Experiment']).max()\n",
    "\n",
    "# Get the convergence performance\n",
    "train_df = df[(df['Train'] == True)]\n",
    "train_df = train_df.groupby(['Agent', 'Experiment', 'Environment', 'Iteration'], as_index=False).rolling(window=window_size, on='Step').mean()\n",
    "train_df = train_df.groupby(['Agent', 'Experiment', 'Iteration', 'Step']).mean()\n",
    "\n",
    "# Get the requests\n",
    "requests_df = df[(df['Train'] == True)].groupby(['Agent', 'Experiment', 'Iteration', 'Step']).sum()\n",
    "\n",
    "performance = []\n",
    "\n",
    "print(f\"\\\\begin{{tabular}}{{c|{'c' * (num_experiments)}}}\")\n",
    "print(f\"        \\\\hline\\\\hline\")\n",
    "print(f\"        \\\\multirow{{2}}{{*}}{{\\\\textbf{{Method}}}} & \\\\multicolumn{{{num_experiments}}}{{c}}{{\\\\textbf{{Environment set}}}} \\\\\\\\\")\n",
    "print('        & ' + ' & '.join([f\"Set {i}\" for i in range(num_experiments)]) + ' \\\\\\\\')\n",
    "\n",
    "for agent in agents:\n",
    "    convergence = []\n",
    "    for exp in range(num_experiments):    \n",
    "        curve = train_df.loc[agent].loc[exp].loc[0]\n",
    "        exceed = curve[curve['Rewards'] > test_df.loc[exp]['Rewards'] * threshold]\n",
    "        if len(exceed) > 0:\n",
    "            index = exceed.index[0]\n",
    "            convergence.append(requests_df.loc[agent].loc[exp].loc[0].loc[index]['Requests'])\n",
    "        else:\n",
    "            convergence.append(np.inf)\n",
    "    performance.append(convergence)\n",
    "    \n",
    "best = np.argmin(performance, axis=0)\n",
    "\n",
    "for a, agent in enumerate(agents):    \n",
    "    string = '& ' + ' & '.join([f\"\\\\textbf{{{c}}}\" if best[i] == a else f\"{c}\" for i, c in enumerate(performance[a])]) + ' \\\\\\\\'\n",
    "    print(f\"        \\\\hline\")\n",
    "    print(f\"        \\\\textit{{{agent.replace('Agent', '')}}} {string.replace('inf', '-')}\")\n",
    "    \n",
    "print(f\"        \\\\hline\\\\hline\")\n",
    "print(f\"    \\\\end{{tabular}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfaceb8",
   "metadata": {},
   "source": [
    "## Collected rewards over the testing simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a524b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = get_dataframe()\n",
    "df = df[df['Train'] == False]\n",
    "\n",
    "agents = ['Independent', 'ClusterFed', 'FlexiFed', 'MultiFedRL']\n",
    "num_experiments = df['Experiment'].max() + 1\n",
    "\n",
    "print(f\"\\\\begin{{tabular}}{{c|{'c' * (num_experiments + 1)}}}\")\n",
    "print(f\"        \\\\hline\\\\hline\")\n",
    "print(f\"        \\\\multirow{{2}}{{*}}{{\\\\textbf{{Method}}}} & \\\\multicolumn{{{num_experiments+1}}}{{c}}{{\\\\textbf{{Environment set}}}} \\\\\\\\\")\n",
    "print('        & ' + ' & '.join([f\"Set {i}\" for i in range(num_experiments)]) + ' & Average \\\\\\\\')\n",
    "\n",
    "performance = []\n",
    "\n",
    "df = df.groupby(['Agent', 'Experiment', 'Iteration']).mean()\n",
    "\n",
    "for agent in agents:\n",
    "    reward_mean = [df.loc[agent].loc[exp]['Rewards'].iloc[0] for exp in range(num_experiments)]\n",
    "    reward_mean.append(sum(reward_mean) / len(reward_mean))\n",
    "    performance.append(reward_mean)\n",
    "\n",
    "best = np.argmax(performance, axis=0)\n",
    "\n",
    "for a, agent in enumerate(agents):\n",
    "    string = '& ' + ' & '.join([f\"\\\\textbf{{{m:.3f}}}\" if best[i] == a else f\"{m:.3f}\" for i, m in enumerate(performance[a])]) + ' \\\\\\\\'\n",
    "    print(f\"        \\\\hline\")\n",
    "    print(f\"        \\\\textit{{{agent.replace('Agent', '')}}} {string}\")\n",
    "print(f\"        \\\\hline\\\\hline\")\n",
    "print(f\"    \\\\end{{tabular}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe()\n",
    "plot = sns.boxplot(\n",
    "    data=df, x='Experiment', y='Reward', hue='Agent',\n",
    "    showfliers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c72a7",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137e259",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fe462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe()\n",
    "\n",
    "df = df[df['Agent'] == 'MultiFed']\n",
    "\n",
    "print(\"\\n---Mean---\")\n",
    "print(df.mean(numeric_only=True))\n",
    "\n",
    "print(\"\\n---Max---\")\n",
    "print(df.max(numeric_only=True))\n",
    "\n",
    "print(\"\\n---Min---\")\n",
    "print(df.min(numeric_only=True))\n",
    "\n",
    "print(\"\\n---Std---\")\n",
    "print(df.std(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a9881",
   "metadata": {},
   "source": [
    "# Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab686a37",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8986a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(json.dumps(configuration, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1040e1",
   "metadata": {},
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a58ab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exp in [exp for exp in sorted(os.listdir(f'.')) if 'Exp' in exp]:\n",
    "    for env in [env for env in sorted(os.listdir(f'./{exp}')) if 'Env' in env]:\n",
    "        print(f'{exp} {env}')\n",
    "        display(Image(filename=f'./{exp}/{env}/{env}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6067d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c23e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image in [image for image in sorted(os.listdir('.')) if '.png' in image]:\n",
    "    print(image)\n",
    "    display(Image(filename=f'{image}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c638a00",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab0c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in open(f'log_{datecode}.txt'):\n",
    "    print(l.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
